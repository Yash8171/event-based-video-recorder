Hello\! I'd be happy to format your project documentation into a downloadable-ready `README.md` file structure.

Since I cannot directly create a downloadable file, I will provide the complete, properly structured Markdown content. You can copy and paste this into a file named `README.md`.

-----

```markdown
# ğŸ¥ Event-Based Video Recorder (AI-Driven Prototype)

A **Python-based prototype system** for **event-driven video recording**, designed for applications like smart fleet monitoring or driver behavior detection.

It continuously buffers live video and saves event-triggered clips (e.g., Drowsiness, Mobile Usage, etc.) along with simulated metadata such as event type, timestamp, and GPS coordinates.

---

## ğŸš€ Features

âœ… **Real-time camera capture** using OpenCV

âœ… **Event-triggered recording** (manual or simulated AI events)

âœ… **Buffered clips** (records **15s before** + **15s after** each event)

âœ… **Metadata storage** in JSON + SQLite (event type, GPS, timestamp)

âœ… **FastAPI backend** to list and download event clips

âœ… **Mock event simulator** to emulate YOLO-based AI detection

âœ… (Optional) Simple **HTML interface** to view saved clips

---

## ğŸ§© Project Structure

```

event\_recorder\_project/
â”œâ”€â”€ clip\_writer.py        \# Handles saving buffered clips with metadata
â”œâ”€â”€ event\_simulator.py    \# Simulates AI event detection (mock YOLO)
â”œâ”€â”€ run\_capture.py        \# Main camera recording and event trigger logic
â”œâ”€â”€ fastapi\_server.py     \# FastAPI backend to view/download clips
â”œâ”€â”€ metadata\_db.py        \# Handles metadata storage in SQLite
â”œâ”€â”€ events.db             \# SQLite database for events
â”œâ”€â”€ metadata.json         \# JSON log for quick event info
â”œâ”€â”€ requirements.txt      \# Python dependencies
â”œâ”€â”€ README.md             \# Project documentation
â”œâ”€â”€ .gitignore            \# Ignores venv, cache, etc.
â””â”€â”€ /clips/               \# Stores saved event video clips

````

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone [https://github.com/Yash8171/event-based-video-recorder.git](https://github.com/Yash8171/event-based-video-recorder.git)
cd event-based-video-recorder
````

### 2ï¸âƒ£ Create & activate virtual environment

```bash
python -m venv venv
# On Windows
venv\Scripts\activate 
# OR On Linux/Mac
source venv/bin/activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

-----

## ğŸ¬ Usage

### â–¶ï¸ Start live capture

```bash
python run_capture.py --source 0
```

Type `event` in the terminal to trigger an event manually.
Each event saves a 30-second clip (15s before + 15s after).
All clips are stored in the `/clips` folder with metadata.

### âš¡ Simulate AI event detection

```bash
python event_simulator.py --avg 30
```

This script randomly triggers simulated events like `Drowsiness`, `NoSeatBelt`, `MobileUsage`, etc., with an average interval of 30 seconds.

### ğŸŒ Run the FastAPI server

```bash
uvicorn fastapi_server:app --reload --port 8000
```

Then open:

  * `http://127.0.0.1:8000/events` â†’ JSON list of saved events
  * `http://127.0.0.1:8000/events/html` â†’ Simple web UI for clips
  * `http://127.0.0.1:8000/download/{filename}` â†’ Download any clip

-----

## ğŸ§  Simulated Metadata

Each event clip saves details like the following example:

```json
{
  "id": 1,
  "filename": "clip_Drowsiness_20251007T104500Z.mp4",
  "event_type": "Drowsiness",
  "timestamp": "2025-10-07T10:45:00Z",
  "gps_lat": 28.5449,
  "gps_lon": 77.3847,
  "duration": 30.0
}
```

-----

## ğŸ¥ Demo Video

[](https://www.google.com/search?q=https://youtu.be/YourVideoID)
**(Replace `YourVideoID` and the link with your actual YouTube or Google Drive video link)**

The video shows:

  * Live camera feed
  * Manual & simulated event triggers
  * Clip saving
  * FastAPI web interface usage

-----

## ğŸ§‘â€ğŸ’» Developer

**Yash Kumar Sahu**

  * B.Tech (CSE), SRMS College of Engineering & Technology
  * ğŸ“ Gurgaon, India
  * [ğŸŒ Portfolio](https://www.google.com/search?q=Your-Portfolio-Link)
  * [ğŸ’¼ LinkedIn](https://www.google.com/search?q=Your-LinkedIn-Link)
  * [ğŸ’» GitHub](https://www.google.com/search?q=Your-GitHub-Link)

-----

## ğŸ“„ License

This project is open-source and available under the [MIT License](https://www.google.com/search?q=LICENSE).

```
```